# A total of 64 32GB GPUs were used for training.
trainer: forces

outputs:
  energy:
    shape: 1
    level: system
  forces:
    irrep_dim: 1
    level: atom
    train_on_free_atoms: True
    eval_on_free_atoms: True

loss_functions:
  - energy:
      fn: mae
      coefficient: 2
  - forces:
      fn: l2mae
      coefficient: 100

evaluation_metrics:
  metrics:
    energy:
      - mae
    forces:
      - mae
      - cosine_similarity
      - magnitude_error
    misc:
      - energy_forces_within_threshold
  primary_metric: forces_mae
  
logger:
    name: tensorboard


model:
  name: scn
  num_interactions: 2
  hidden_channels: 16
  sphere_channels: 8
  sphere_channels_reduce: 8
  num_sphere_samples: 8
  num_basis_functions: 8
  distance_function: "gaussian"
  show_timing_info: False
  max_num_neighbors: 40
  cutoff: 8.0
  lmax: 4
  num_bands: 2
  use_grid: True
  regress_forces: True
  use_pbc: True
  basis_width_scalar: 2.0
  otf_graph: True

optim:
  batch_size: 2
  eval_batch_size: 1
  num_workers: 2
  lr_initial: 0.0004
  optimizer: AdamW
  optimizer_params: {"amsgrad": True}
  eval_every: 5000
  lr_gamma: 0.3
  lr_milestones: # epochs at which lr_initial <- lr_initial * lr_gamma
    - 260000
    - 340000
    - 420000
    - 500000
    - 800000
    - 1000000
  warmup_steps: 100
  warmup_factor: 0.2
  max_epochs: 12
  clip_grad_norm: 100
  ema_decay: 0.999
