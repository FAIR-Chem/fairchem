{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee85286-fcfe-452b-9017-7c0bce8168bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ocpmodels.common.relaxation.ase_utils import OCPCalculator\n",
    "import ase.io\n",
    "from ase.optimize import BFGS\n",
    "\n",
    "from ocdata.core import Adsorbate, AdsorbateSlabConfig, Bulk, Slab\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from ocdata.utils import DetectTrajAnomaly\n",
    "from ocdata.utils.vasp import write_vasp_input_files\n",
    "\n",
    "# Optional - see below\n",
    "import numpy as np\n",
    "from dscribe.descriptors import SOAP\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from x3dase.visualize import view_x3d_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a774b7f5-e662-4959-9202-660d7bcaaacd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Enumerate the adsorbate-slab configurations to run relaxations on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b10e112-25be-43e2-8f7d-0cda02187746",
   "metadata": {},
   "source": [
    "Be sure to set the path to the bulk and adsorbate pickle files in `ocdata/configs/paths.py` or pass the paths as an argument. The database pickles can be found in `ocdata/databases/pkls`. AdsorbML incorporates random placement, which is especially useful for more complicated adsorbates which may have many degrees of freedom. I have opted sample a few random placements and a few heuristic. Here I am using *CO on copper (1,1,1) as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7132ce-9536-4b5f-a7f7-cf42a528b312",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_src_id = \"mp-30\"\n",
    "adsorbate_smiles = \"*CO\"\n",
    "\n",
    "bulk = Bulk(bulk_src_id_from_db = bulk_src_id, bulk_db_path = \"your-path-here.pkl\")\n",
    "adsorbate = Adsorbate(adsorbate_smiles_from_db=adsorbate_smiles, adsorbate_db_path = \"your-path-here.pkl\")\n",
    "slabs = Slab.from_bulk_get_specific_millers(bulk = bulk, specific_millers=(1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d834adae-9d08-4e6f-9ada-23fc7c6b67d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform heuristic placements\n",
    "heuristic_adslabs = AdsorbateSlabConfig(slabs[0], adsorbate, mode=\"heuristic\")\n",
    "\n",
    "# Perform random placements\n",
    "# (for AdsorbML we use `num_sites = 100` but we will use 4 for brevity here)\n",
    "random_adslabs = AdsorbateSlabConfig(slabs[0], adsorbate, mode=\"random_site_heuristic_placement\", num_sites = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df1a82-46b1-4ab1-8c2c-e4983fe31747",
   "metadata": {},
   "source": [
    "# Run ML relaxations:\n",
    "\n",
    "There are 2 options for how to do this.\n",
    " 1. Using `OCPCalculator` as the calculator within the ASE framework\n",
    " 2. By writing objects to lmdb and relaxing them using `main.py` in the ocp repo\n",
    " \n",
    "(1) is really only adequate for small stuff and it is what I will show here, but if you plan to run many relaxations, you should definitely use (2). More details about writing lmdbs has been provided [here](https://github.com/Open-Catalyst-Project/ocp/blob/main/tutorials/lmdb_dataset_creation.ipynb) - follow the IS2RS/IS2RE instructions. And more information about running relaxations once the lmdb has been written is [here](https://github.com/Open-Catalyst-Project/ocp/blob/main/TRAIN.md#initial-structure-to-relaxed-structure-is2rs).\n",
    "\n",
    "You need to provide the calculator with a path to a model checkpoint file. That can be downloaded [here](https://github.com/Open-Catalyst-Project/ocp/blob/main/MODELS.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe297959-7de2-4e54-bca6-562b230de12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"your-path-here.pt\"\n",
    "os.makedirs(f\"data/{bulk}_{adsorbate}\", exist_ok=True)\n",
    "\n",
    "# Define the calculator\n",
    "calc = OCPCalculator(checkpoint=checkpoint_path) # if you have a gpu, add `cpu=False` to speed up calculations\n",
    "\n",
    "adslabs = [*heuristic_adslabs.atoms_list, *random_adslabs.atoms_list]\n",
    "# Set up the calculator\n",
    "for idx, adslab in enumerate(adslabs):\n",
    "    adslab.calc = calc\n",
    "    opt = BFGS(adslab, trajectory=f\"data/{bulk}_{adsorbate}/{idx}.traj\")\n",
    "    opt.run(fmax=0.05, steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd48f41-52d9-475f-9b11-ec48fc0dc529",
   "metadata": {},
   "source": [
    "# Parse the trajectories and post-process\n",
    "\n",
    "As a post-processing step we check to see if:\n",
    "1. the adsorbate desorbed\n",
    "2. the adsorbate disassociated\n",
    "3. the adsorbate intercalated\n",
    "4. the surface has changed\n",
    "\n",
    "We check these because the effect our referencing scheme and may result in erroneous energies. For (4), the relaxed surface should really be supplied as well. It will be necessary when correcting the SP / RX energies later. Since we don't have it here, we will ommit supplying it, and the detector will instead compare the initial and final slab from the adsorbate-slab relaxation trajectory. If a relaxed slab is provided, the detector will compare it and the slab after the adsorbate-slab relaxation. The latter is more correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d005711-c702-4362-b930-f5bb8a6bbc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over trajs to extract results\n",
    "results = []\n",
    "for file in glob(f\"data/{bulk}_{adsorbate}/*.traj\"):\n",
    "    rx_id = file.split(\"/\")[-1].split(\".\")[0]\n",
    "    traj = ase.io.read(file, \":\")\n",
    "    \n",
    "    # Check to see if the trajectory is anomolous\n",
    "    detector = DetectTrajAnomaly(traj[0], traj[-1], traj[0].get_tags())\n",
    "    anom = (\n",
    "        detector.is_adsorbate_dissociated()\n",
    "        or detector.is_adsorbate_desorbed()\n",
    "        or detector.has_surface_changed()\n",
    "        or detector.is_adsorbate_intercalated()\n",
    "    )\n",
    "    rx_energy = traj[-1].get_potential_energy()\n",
    "    results.append({\"relaxation_idx\": rx_id, \"relaxed_atoms\": traj[-1],\n",
    "                    \"relaxed_energy_ml\": rx_energy, \"anomolous\": anom})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad52f34b-a885-4950-b040-b782acf0b899",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cec27e4-ea87-468b-8088-389ba7635035",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrap anomalies\n",
    "df = df[~df.anomolous].copy().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b71df2-8742-4789-bbbf-1654a5a174e4",
   "metadata": {},
   "source": [
    "# (Optional) Deduplicate structures\n",
    "We may have enumerated very similar structures or structures may have relaxed to the same configuration. For this reason, it is advantageous to cull systems if they are very similar. This results in marginal improvements in the recall metrics we calculated for AdsorbML, so it wasnt implemented there. It is, however, a good way to prevent wasteful VASP calculations. You can also imagine that if we would have enumerated 1000 configs per slab adsorbate combo rather than 100 for AdsorbML, it is more likely that having redundant systems would reduce performance, so its a good thing to keep in mind. This may be done by eye for a small number of systems, but with many systems it is easier to use an automated approach. Here is an example of one such approach, which uses a SOAP descriptor to find similar systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd356d23-4b5e-4aaf-8f04-eb99ab96394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the configs and their energies\n",
    "def deduplicate(configs_for_deduplication: list,\n",
    "                adsorbate_binding_index: int,\n",
    "                cosine_similarity = 1e-3,\n",
    "               ):\n",
    "    \"\"\"\n",
    "    A function that may be used to deduplicate similar structures.\n",
    "    Among duplicate entries, the one with the lowest energy will be kept.\n",
    "    \n",
    "    Args:\n",
    "        configs_for_deduplication: a list of ML relaxed adsorbate-\n",
    "            surface configurations.\n",
    "        cosine_similarity: The cosine simularity value above which,\n",
    "            configurations are considered duplicate.\n",
    "            \n",
    "    Returns:\n",
    "        (list): the indices of configs which should be kept as non-duplicate\n",
    "    \"\"\"\n",
    "    \n",
    "    energies_for_deduplication = np.array([atoms.get_potential_energy() for atoms in configs_for_deduplication])\n",
    "    # Instantiate the soap descriptor\n",
    "    soap = SOAP(\n",
    "        species=np.unique(configs_for_deduplication[0].get_chemical_symbols()),\n",
    "        rcut=2.0,\n",
    "        nmax=6,\n",
    "        lmax=3,\n",
    "        periodic=True,\n",
    "    )\n",
    "    #Figure out which index cooresponds to \n",
    "    ads_len = list(configs_for_deduplication[0].get_tags()).count(2)\n",
    "    position_idx = -1*(ads_len-adsorbate_binding_index)\n",
    "    # Iterate over the systems to get the SOAP vectors\n",
    "    soap_desc = []\n",
    "    for config in configs_for_deduplication:\n",
    "        soap_ex = soap.create(config, positions=[position_idx])\n",
    "        soap_desc.extend(soap_ex)\n",
    "\n",
    "    soap_descs = np.vstack(soap_desc)\n",
    "\n",
    "    #Use euclidean distance to assess similarity\n",
    "    distance = squareform(pdist(soap_descs, metric=\"cosine\"))\n",
    "\n",
    "    bool_matrix = np.where(distance <= cosine_similarity, 1, 0)\n",
    "    # For configs that are found to be similar, just keep the lowest energy one\n",
    "    idxs_to_keep = []\n",
    "    pass_idxs = []\n",
    "    for idx, row in enumerate(bool_matrix):\n",
    "        if idx in pass_idxs:\n",
    "            continue\n",
    "            \n",
    "        elif sum(row) == 1:\n",
    "            idxs_to_keep.append(idx)\n",
    "        else:\n",
    "            same_idxs = [row_idx for row_idx, val in enumerate(row) if val == 1]\n",
    "            pass_idxs.extend(same_idxs)\n",
    "            # Pick the one with the lowest energy by ML\n",
    "            min_e = min(energies_for_deduplication[same_idxs])\n",
    "            idxs_to_keep.append(list(energies_for_deduplication).index(min_e))\n",
    "    return idxs_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dfa417-02fe-4c66-b06a-aeab56f702e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_for_deduplication =  df.relaxed_atoms.tolist()\n",
    "idxs_to_keep = deduplicate(configs_for_deduplication, adsorbate.binding_indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729be6f0-22b9-4d10-860b-39d7d8209ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip through your configurations to check them out (and make sure deduplication looks good)\n",
    "print(idxs_to_keep)\n",
    "view_x3d_n(configs_for_deduplication[2].repeat((2,2,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd2c12e-d355-4213-b100-81a4054cbbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[idxs_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b87513-8397-4216-a567-ee81a25cef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_e_values = np.round(df.sort_values(by = \"relaxed_energy_ml\").relaxed_energy_ml.tolist()[0:5],3)\n",
    "print(f\"The lowest 5 energies are: {low_e_values}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe926290-3bec-406c-83d3-fcb2eee76c0a",
   "metadata": {},
   "source": [
    "# Write VASP input files\n",
    "\n",
    "This assumes you have access to VASP pseudopotentials. The default VASP flags (which are equivalent to those used to make OC20) are located in `ocdata.utils.vasp`. Alternatively, you may pass your own vasp flags to the `write_vasp_input_files` function as `vasp_flags`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baeaa19-bc79-424c-8d1f-9b4a066a486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the 5 systems with the lowest energy\n",
    "configs_for_dft = df.sort_values(by = \"relaxed_energy_ml\").relaxed_atoms.tolist()[0:5]\n",
    "config_idxs = df.sort_values(by = \"relaxed_energy_ml\").relaxation_idx.tolist()[0:5]\n",
    "\n",
    "# Write the inputs\n",
    "for idx, config in enumerate(configs_for_dft):\n",
    "    os.mkdir(f\"data/{config_idxs[idx]}\")\n",
    "    write_vasp_input_files(config, outdir = f\"data/{config_idxs[idx]}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
