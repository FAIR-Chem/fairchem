{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "natural-debut",
   "metadata": {},
   "source": [
    "### OCP Data Preprocessing Tutorial\n",
    "\n",
    "\n",
    "This notebook provides an overview of converting ASE Atoms objects to PyTorch Geometric Data objects. To better understand the raw data contained within OC20, check out the following tutorial first: https://github.com/Open-Catalyst-Project/ocp/blob/master/docs/source/tutorials/data_playground.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "metric-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ocpmodels.preprocessing import AtomsToGraphs\n",
    "import ase.io\n",
    "from ase.build import bulk\n",
    "from ase.build import fcc100, add_adsorbate, molecule\n",
    "from ase.constraints import FixAtoms\n",
    "from ase.calculators.emt import EMT\n",
    "from ase.optimize import BFGS\n",
    "import torch\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-meaning",
   "metadata": {},
   "source": [
    "### Generate toy dataset: Relaxation of CO on Cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "american-swiss",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adslab = fcc100(\"Cu\", size=(2, 2, 3))\n",
    "ads = molecule(\"CO\")\n",
    "add_adsorbate(adslab, ads, 3, offset=(1, 1))\n",
    "cons = FixAtoms(indices=[atom.index for atom in adslab if (atom.tag == 3)])\n",
    "adslab.set_constraint(cons)\n",
    "adslab.center(vacuum=13.0, axis=2)\n",
    "adslab.set_pbc(True)\n",
    "adslab.set_calculator(EMT())\n",
    "dyn = BFGS(adslab, trajectory=\"CuCO_adslab.traj\", logfile=None)\n",
    "dyn.run(fmax=0, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "guilty-stuff",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n"
     ]
    }
   ],
   "source": [
    "raw_data = ase.io.read(\"CuCO_adslab.traj\", \":\")\n",
    "print(len(raw_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-joseph",
   "metadata": {},
   "source": [
    "### Convert Atoms object to Data object\n",
    "\n",
    "The AtomsToGraphs class takes in several arguments to control how Data objects created:\n",
    "\n",
    "- max_neigh (int):   Maximum number of neighbors a given atom is allowed to have, discarding the furthest\n",
    "- radius (float):      Cutoff radius to compute nearest neighbors around\n",
    "- r_energy (bool):    Write energy to Data object\n",
    "- r_forces (bool):    Write forces to Data object\n",
    "- r_distances (bool): Write distances between neighbors to Data object\n",
    "- r_edges (bool):     Write neigbhor edge indices to Data object\n",
    "- r_fixed (bool):     Write indices of fixed atoms to Data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "realistic-bunch",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "a2g = AtomsToGraphs(\n",
    "    max_neigh=50,\n",
    "    radius=6,\n",
    "    r_energy=True,\n",
    "    r_forces=True,\n",
    "    r_distances=False,\n",
    "    r_edges=True,\n",
    "    r_fixed=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "unsigned-headset",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data_objects = a2g.convert_all(raw_data, disable_tqdm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "enclosed-future",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(atomic_numbers=[14], cell=[1, 3, 3], cell_offsets=[636, 3], edge_index=[2, 636], fixed=[14], force=[14, 3], natoms=14, pos=[14, 3], y=3.9893144106684715)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data_objects[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ignored-offering",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29., 29., 29., 29., 29., 29., 29., 29., 29., 29., 29., 29.,  8.,  6.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.atomic_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "conventional-liability",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.1053,  0.0000,  0.0000],\n",
       "         [ 0.0000,  5.1053,  0.0000],\n",
       "         [ 0.0000,  0.0000, 32.6100]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ruled-fundamental",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  2,  ...,  4,  6,  3],\n",
       "        [ 0,  0,  0,  ..., 13, 13, 13]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index #neighbor idx, source idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cooperative-warehouse",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "satisfactory-petroleum",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3250e-15,  1.8807e-15,  1.1354e-01],\n",
       "        [ 9.0249e-16,  1.1050e-15,  1.1344e-01],\n",
       "        [ 5.2822e-15,  2.9421e-15,  1.1344e-01],\n",
       "        [-3.4399e-17,  6.2746e-17,  1.1294e-01],\n",
       "        [-8.5221e-03, -8.5221e-03, -1.1496e-02],\n",
       "        [ 8.5221e-03, -8.5221e-03, -1.1496e-02],\n",
       "        [-8.5221e-03,  8.5221e-03, -1.1496e-02],\n",
       "        [ 8.5221e-03,  8.5221e-03, -1.1496e-02],\n",
       "        [ 8.5001e-16, -8.4308e-16, -1.0431e-01],\n",
       "        [-2.0583e-15, -4.5797e-16, -6.6610e-02],\n",
       "        [-5.5511e-17, -5.8287e-16, -6.6610e-02],\n",
       "        [-1.7780e-15, -2.5274e-15, -3.3250e-01],\n",
       "        [-4.2690e-19, -8.6059e-19, -3.4247e-01],\n",
       "        [-4.3368e-17, -2.4286e-17,  5.0512e-01]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "broad-rouge",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000, 13.0000],\n",
       "        [ 2.5527,  0.0000, 13.0000],\n",
       "        [ 0.0000,  2.5527, 13.0000],\n",
       "        [ 2.5527,  2.5527, 13.0000],\n",
       "        [ 1.2763,  1.2763, 14.8050],\n",
       "        [ 3.8290,  1.2763, 14.8050],\n",
       "        [ 1.2763,  3.8290, 14.8050],\n",
       "        [ 3.8290,  3.8290, 14.8050],\n",
       "        [ 0.0000,  0.0000, 16.6100],\n",
       "        [ 2.5527,  0.0000, 16.6100],\n",
       "        [ 0.0000,  2.5527, 16.6100],\n",
       "        [ 2.5527,  2.5527, 16.6100],\n",
       "        [ 2.5527,  2.5527, 19.6100],\n",
       "        [ 2.5527,  2.5527, 18.4597]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "coupled-orchestra",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9893144106684715"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-treat",
   "metadata": {},
   "source": [
    "### Adding additional info to your Data objects\n",
    "\n",
    "In addition to the above information, the OCP repo requires several other pieces of information for your data to work\n",
    "with the provided trainers:\n",
    "\n",
    "- sid (int): A unique identifier for a particular system. Does not affect your model performance, used for prediction saving \n",
    "- fid (int) (S2EF only): If training for the S2EF task, your data must also contain a unique frame identifier for atoms objects coming from the same system.\n",
    "- tags (tensor): Tag information - 0 for adsorbate, 1 for surface, 2 for subsurface. Optional, can be used for training.\n",
    "\n",
    "\n",
    "Other information may be added her as well if you choose to incorporate other information in your models/frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "simple-champion",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data_objects = []\n",
    "for idx, system in enumerate(raw_data):\n",
    "    data = a2g.convert(system)\n",
    "    data.fid = idx\n",
    "    data.sid = 0 # All data points come from the same system, arbitrarly define this as 0\n",
    "    data_objects.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "subjective-present",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(atomic_numbers=[14], cell=[1, 3, 3], cell_offsets=[635, 3], edge_index=[2, 635], fid=100, fixed=[14], force=[14, 3], natoms=14, pos=[14, 3], sid=0, y=3.9683558933957697)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data_objects[100]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "thick-geology",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "elegant-amendment",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "civil-better",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to convert from PyTorch Geometric input to GROVER input:\n",
    "def convert_input(data, args):\n",
    "    \"\"\"\n",
    "        :param data: data as PyTorch geometric object\n",
    "        :param f_atoms: the atom features, num_atoms * atom_dim\n",
    "        :param f_bonds: the bond features, num_bonds * bond_dim\n",
    "        :param a2b: mapping from atom index to incoming bond indices. \n",
    "        :param a2a: mapping from atom index to its neighbors. num_atoms * max_num_bonds\n",
    "        :param b2a: mapping from bond index to the index of the atom the bond is coming from.\n",
    "        :param b2revb: mapping from bond index to the index of the reverse bond.\n",
    "        :return: batch = (f_atoms, f_bonds, a2b, a2a, b2a, b2revb)\n",
    "    \"\"\"\n",
    "    # Per atom features: (atomic_number, pos_x, pos_y, pos_z)\n",
    "#     f_atoms = torch.stack((data.atomic_numbers.long(), data.pos[:,0], data.pos[:,1], data.pos[:,2]), 1)\n",
    "    # Per edge features (calculatedby atomic distances in model forward pass)\n",
    "#     f_bonds = data.edge_attr\n",
    "    f_bonds = torch.zeros((100))\n",
    "    f_atoms = torch.zeros((100))\n",
    "    args.undirected = False\n",
    "\n",
    "    # OLD CODE FOR CALCULATING MAPPINGS\n",
    "    num_atoms_total = int(torch.sum(data.natoms))\n",
    "    a2a = [[] for j in range(num_atoms_total)]  # List of lists - Dynamically append neighbors for a given atom\n",
    "    a2b = [[] for j in range(num_atoms_total)]  # List of lists - Dynamically append edges for a given atom\n",
    "    b2a = torch.zeros((data.edge_index.shape[1],)).long() # (num_edges, ) - One originating atom per edge\n",
    "    b2revb = torch.zeros((data.edge_index.shape[1],)).long()  # (num_edges, ) - One reverse bond per bond\n",
    "    rev_edges = {}  # Dict of lists for each (from_atom, to_atom) pair, saving edge numbers\n",
    "\n",
    "    # Loop through every edge in the graph\n",
    "    for i in range(data.edge_index.shape[1]):\n",
    "        from_atom = int(data.edge_index[0][i])\n",
    "        to_atom = int(data.edge_index[1][i])\n",
    "        a2a[from_atom].append(to_atom)  # Mark b as neighbor of a\n",
    "        a2b[to_atom].append(i)  # Mark bond i as incoming bond to atom a\n",
    "        b2a[i] = from_atom  # Mark a as atom where bond i is originating\n",
    "        key = frozenset({to_atom, from_atom})\n",
    "        if (key not in rev_edges):  # If the edge from these two atoms has not been seen yet\n",
    "            rev_edges[key] = []  # Declare it as a list (so we can keep track of the edge numbers)\n",
    "        rev_edges[key].append(i)  # Append the edge number to the list\n",
    "\n",
    "    # Iterate through and set b2revb with reverse bonds\n",
    "    for atoms, edges in rev_edges.items():\n",
    "        if(len(edges) == 2):\n",
    "            b2revb[edges[0]] = edges[1]\n",
    "            b2revb[edges[1]] = edges[0]\n",
    "        elif(len(edges) == 4): # In the case of duplicate edges, they are grouped together in this order\n",
    "            b2revb[edges[0]] = edges[2]\n",
    "            b2revb[edges[2]] = edges[0]\n",
    "            b2revb[edges[1]] = edges[3]\n",
    "            b2revb[edges[3]] = edges[1]\n",
    "        elif(len(edges) == 1):\n",
    "            args.undirected = False\n",
    "\n",
    "    # Convert list of lists for a2a and a2b into tensor: (num_nodes, max_edges)\n",
    "    # Trim length to max number of edges seen in the data (should be capped by 50 but not always in practice)\n",
    "    a2a_pad = len(max(a2a, key=len))\n",
    "    a2b_pad = len(max(a2b, key=len))\n",
    "    # -1 is not a valid atom or edge index so we pad with this\n",
    "    a2a = torch.tensor([i + [0] * (a2a_pad - len(i)) for i in a2a])\n",
    "    a2b = torch.tensor([i + [0] * (a2b_pad - len(i)) for i in a2b])\n",
    "\n",
    "    batch = (f_atoms, f_bonds, a2b, a2a, b2a, b2revb)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "simplified-durham",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2a2 len:  7020\n",
      "data.edge_index:  tensor([[   3,    4,    7,  ..., 7016, 7017, 7018],\n",
      "        [   0,    0,    0,  ..., 7019, 7019, 7019]], device='cuda:0')\n",
      "data.natoms:  tensor([118,  77,  47,  54, 100,  78,  80,  33,  87,  57,  84, 132,  85,  29,\n",
      "         61,  55, 132, 102, 103,  61,  33,  40,  70,  87,  78, 122,  41,  45,\n",
      "         83, 136,  68,  77,  36,  98,  62,  74, 118,  80,  54,  70,  30,  78,\n",
      "         76,  71,  36,  27,  46,  60,  79,  57,  88,  86, 100,  53,  56,  38,\n",
      "        106,  12,  80,  71, 149,  41,  87,  70,  64,  52,  97,  54, 102,  56,\n",
      "         52, 111,  37,  39,  65,  36,  44, 136,  74,  77, 103,  99,  40,  63,\n",
      "        104, 102,  28,  63,  50,  51,  38,  67,  52,  73,  70,  44,  76,  78,\n",
      "         39,  41], device='cuda:0')\n",
      "data.batch:  tensor([ 0,  0,  0,  ..., 99, 99, 99], device='cuda:0')\n",
      "len data batch:  7021\n",
      "natoms total:  tensor(7021, device='cuda:0')\n",
      "natoms entries that are equal to zero:  0\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "path = \"/home/etower/ocp/logs/data_dump/80hidden\"\n",
    "a2a2 = torch.load(path + \"/a2a2.pt\").cpu()\n",
    "a2b2 = torch.load(path + \"/a2b2.pt\").cpu()\n",
    "b2a2 = torch.load(path + \"/b2a2.pt\").cpu()\n",
    "data = torch.load(path + \"/data.pt\")\n",
    "print(\"a2a2 len: \", len(a2a2))\n",
    "\n",
    "args = Namespace()\n",
    "print(\"data.edge_index: \", data.edge_index)\n",
    "print(\"data.natoms: \", data.natoms)\n",
    "print(\"data.batch: \", data.batch)\n",
    "print(\"len data batch: \", len(data.batch))\n",
    "print(\"natoms total: \", torch.sum(data.natoms))\n",
    "print(\"natoms entries that are equal to zero: \", len(data.natoms[data.natoms==0]))\n",
    "\n",
    "batch = convert_input(data, args)\n",
    "f_atoms, f_bonds, a2b, a2a, b2a, b2revb = batch\n",
    "a2a = a2a.cpu()\n",
    "a2b = a2b.cpu()\n",
    "b2a = b2a.cpu()\n",
    "data.edge_index = data.edge_index.cpu()\n",
    "\n",
    "\n",
    "# edge_index[0] = data.edge_index[0].split(tuple(data.batch))\n",
    "# edge_index[1] = data.edge_index[1].split(tuple(data.batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "talented-garage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW WAY OF CALCULATING a2a2, a2b2, b2a2: Add padding to match num_atoms_total length\n",
    "# Skip to use a2a2 from ocp run/log dump\n",
    "\n",
    "num_atoms_total = int(torch.sum(data.natoms))\n",
    "\n",
    "# Calculate atom to neighboring atom mappings (a2a)\n",
    "trans = data.edge_index.T\n",
    "sorted_index = trans[trans[:, 0].sort()[1]]  # Sort by column zero (from_node), index based off\n",
    "sorted_index = sorted_index.T\n",
    "\n",
    "out, counts = torch.unique(sorted_index[0], return_counts=True)  # counts for each atom\n",
    "if len(out) - 1 != out[-1]: # If they are the correct length then we don't need to add 0's\n",
    "    ascending = torch.arange(out[-1] + 1).cuda() # Ascending order list [0, 1, ... n] where n is last elem in out\n",
    "    missing = [ elem not in out for elem in ascending ] # Missing elements in out\n",
    "    for elem in numpy.nonzero(missing)[0]:\n",
    "        i = int(elem)\n",
    "        counts = torch.cat((counts[:i], torch.tensor((0,)).cuda(), counts[i:])) # Add zero to count\n",
    "a2a2 = sorted_index[1].split(counts.tolist())  # Index into to_bonds with these indices\n",
    "a2a2 = torch.nn.utils.rnn.pad_sequence(list(a2a2), batch_first=True, padding_value=0) # Pad with zeros into a tensor\n",
    "# New line\n",
    "\n",
    "if len(a2a2 < num_atoms_total):\n",
    "    a2a2 = torch.cat((a2a2, torch.zeros((1, a2a2.shape[1])))) # Add extra entry of zeros to match dimensions\n",
    "\n",
    "# Calculate outgoing bond to atom mappings (b2a)\n",
    "b2a2 = data.edge_index[0].type(torch.LongTensor)\n",
    "\n",
    "# Calculate atom to incoming bond mappings (a2b)\n",
    "out, counts = torch.unique(data.edge_index[1], return_counts=True)\n",
    "if len(out) - 1 != out[-1]: # If they are the correct length then we don't need to add 0's\n",
    "    ascending = torch.arange(out[-1] + 1).cuda() # Ascending order list [0, 1, ... n] where n is last elem in out\n",
    "    missing = [ elem not in out for elem in ascending ] # Missing elements in out\n",
    "    for elem in numpy.nonzero(missing)[0]:\n",
    "        i = int(elem)\n",
    "        counts = torch.cat((counts[:i], torch.tensor((0,)).cuda(), counts[i:])) # Add zero to count\n",
    "\n",
    "a2b2 = torch.split(torch.arange(len(data.edge_index[1])), tuple(counts))\n",
    "a2b2 = torch.nn.utils.rnn.pad_sequence(a2b2, batch_first=True, padding_value=0) # Pad with zeros into a tensor\n",
    "\n",
    "# New line\n",
    "if len(a2b2 < num_atoms_total): \n",
    "    a2b2 = torch.cat((a2b2, torch.zeros((1, a2b2.shape[1])))) # Add extra entry of zeros to match dimensions\n",
    "\n",
    "a2a2 = a2a2.cpu().type(torch.LongTensor)\n",
    "a2b2 = a2b2.cpu().type(torch.LongTensor)\n",
    "b2a2 = b2a2.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "cultural-garden",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2a2 shape:  torch.Size([7021, 66])\n",
      "a2a shape:   torch.Size([7021, 66])\n",
      "a2b2 shape:  torch.Size([7021, 33])\n",
      "a2b shape:   torch.Size([7021, 33])\n",
      "b2a2 shape:  torch.Size([146009])\n",
      "b2a shape:   torch.Size([146009])\n",
      "edge_index shape:  torch.Size([2, 146009])\n",
      "\n",
      "len unique in edge index[0]:  7020\n",
      "len unique in edge index[1]:  7020\n",
      "out0:  tensor([   0,    1,    2,  ..., 7017, 7018, 7019], device='cuda:0')\n",
      "out1:  tensor([   0,    1,    2,  ..., 7017, 7018, 7019], device='cuda:0')\n",
      "len unique total:  7020\n"
     ]
    }
   ],
   "source": [
    "print(\"a2a2 shape: \", a2a2.shape)\n",
    "print(\"a2a shape:  \", a2a.shape)\n",
    "print(\"a2b2 shape: \", a2b2.shape)\n",
    "print(\"a2b shape:  \", a2b.shape)\n",
    "print(\"b2a2 shape: \", b2a2.shape)\n",
    "print(\"b2a shape:  \", b2a.shape)\n",
    "print(\"edge_index shape: \", edge_index.shape)\n",
    "\n",
    "out0, counts0 = torch.unique(edge_index[0], return_counts=True) # counts for each atom\n",
    "print(\"\\nlen unique in edge index[0]: \", len(out0))\n",
    "\n",
    "out1, counts1 = torch.unique(edge_index[1], return_counts=True) # counts for each atom\n",
    "print(\"len unique in edge index[1]: \", len(out1))\n",
    "\n",
    "print(\"out0: \", out0)\n",
    "print(\"out1: \", out1)\n",
    "\n",
    "out_total =  np.union1d(out0.cpu(), out1.cpu())\n",
    "print(\"len unique total: \", len(out_total))\n",
    "\n",
    "# Count unique atoms manually with for loop\n",
    "out0_manual = {}\n",
    "out1_manual = {}\n",
    "\n",
    "missing_a2a = {}\n",
    "missing_a2a2 = {}\n",
    "# Check that a2a is correct: that for every atom, it is actually connected with the corresponding one\n",
    "for i in range(len(data.edge_index[0])):\n",
    "    from_atom = int(data.edge_index[0][i].cpu())\n",
    "    to_atom = int(data.edge_index[1][i].cpu())\n",
    "    if to_atom not in a2a[from_atom]:\n",
    "        if from_atom not in missing_a2a:\n",
    "            missing_a2a[from_atom] = []\n",
    "        missing_a2a[from_atom].append(to_atom)\n",
    "    if to_atom not in a2a2[from_atom]:\n",
    "        if from_atom not in missing_a2a2:\n",
    "            missing_a2a2[from_atom] = []\n",
    "        missing_a2a2[from_atom].append(to_atom)\n",
    "    if from_atom not in out0_manual:\n",
    "        out0_manual[from_atom] = 1\n",
    "    if to_atom not in out1_manual:\n",
    "        out1_manual[to_atom] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "republican-return",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7020\n",
      "7020\n",
      "7020\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of unique elements in edge index manually\n",
    "out0_m = list(out0_manual)\n",
    "out0_m = [ int(i) for i in out0_m ]\n",
    "\n",
    "out1_m = list(out1_manual)\n",
    "out1_m = [ int(i) for i in out1_m ]\n",
    "\n",
    "sort_0 = np.sort(out0_m)\n",
    "print(len(sort_0))\n",
    "print(len(out1_m))\n",
    "print(len(np.union1d(sort_0, out1_m)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "institutional-peeing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing a2a:  {}\n",
      "missing a2a2:  {}\n",
      "\n",
      "b2a and b2a2 equal?  True\n",
      "b2a shape:  torch.Size([146009])\n",
      "b2a2 shape:  torch.Size([146009])\n",
      "\n",
      "a2b and a2b2 equal?  True\n",
      "a2b shape:  torch.Size([7021, 33])\n",
      "a2b2 shape:  torch.Size([7021, 33])\n",
      "\n",
      "a2a and a2a2 equal?  False\n",
      "a2a and a2a2 equal counts:  True\n",
      "a2a count:  tensor([317396,      9,      8,  ...,     16,     13,     17])\n",
      "a2a2 count:  tensor([317396,      9,      8,  ...,     16,     13,     17])\n",
      "a2a shape:  torch.Size([7021, 66])\n",
      "a2a2 shape:  torch.Size([7021, 66])\n"
     ]
    }
   ],
   "source": [
    "print(\"missing a2a: \", missing_a2a)\n",
    "print(\"missing a2a2: \", missing_a2a2)\n",
    "\n",
    "print(\"\\nb2a and b2a2 equal? \", torch.equal(b2a, b2a2))\n",
    "print(\"b2a shape: \", b2a.shape)\n",
    "print(\"b2a2 shape: \", b2a2.shape)\n",
    "\n",
    "print(\"\\na2b and a2b2 equal? \", torch.equal(a2b, a2b2))\n",
    "print(\"a2b shape: \", a2b.shape)\n",
    "print(\"a2b2 shape: \", a2b2.shape)\n",
    "\n",
    "print(\"\\na2a and a2a2 equal? \", torch.equal(a2a, a2a2))\n",
    "print(\"a2a and a2a2 equal counts: \",\n",
    "      torch.equal(torch.unique(a2a, return_counts=True)[1], torch.unique(a2a2, return_counts=True)[1]))\n",
    "print(\"a2a count: \", torch.unique(a2a, return_counts=True)[1])\n",
    "print(\"a2a2 count: \", torch.unique(a2a2, return_counts=True)[1])\n",
    "print(\"a2a shape: \", a2a.shape)\n",
    "print(\"a2a2 shape: \", a2a2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "characteristic-marking",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2a[-1]:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "a2a[-2]:   tensor([6980, 6981, 6986, 6987, 6992, 6993, 6994, 6995, 7004, 7005, 7010, 7012,\n",
      "        7016, 7017, 7018,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0])\n",
      "a2a2[-1]:  tensor([6980, 6981, 6986, 6987, 6992, 6993, 6994, 6995, 7004, 7005, 7010, 7012,\n",
      "        7016, 7017, 7018,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0])\n",
      "\n",
      "a2a[-3]:   tensor([6986, 6987, 6988, 6992, 6993, 6994, 7004, 7005, 7010, 7011, 7016, 7017,\n",
      "        7019,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0])\n",
      "a2a2[-2]:  tensor([6986, 6987, 6988, 6992, 6993, 6994, 7004, 7005, 7010, 7011, 7016, 7017,\n",
      "        7019,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "print(\"a2a[-1]: \", a2a[-1])\n",
    "print(\"\\na2a[-2]:  \", a2a[-2])\n",
    "print(\"a2a2[-1]: \",a2a2[-1])\n",
    "\n",
    "print(\"\\na2a[-3]:  \", a2a[-3])\n",
    "print(\"a2a2[-2]: \", a2a2[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "portuguese-compound",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2b[-1]:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "a2b[-2] tensor([145992, 145993, 145994, 145995, 145996, 145997, 145998, 145999, 146000,\n",
      "        146001, 146002, 146003, 146004, 146005, 146006, 146007, 146008,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0])\n",
      "a2b2[-1]:  tensor([145992, 145993, 145994, 145995, 145996, 145997, 145998, 145999, 146000,\n",
      "        146001, 146002, 146003, 146004, 146005, 146006, 146007, 146008,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0])\n",
      "\n",
      "a2b[-3] tensor([145979, 145980, 145981, 145982, 145983, 145984, 145985, 145986, 145987,\n",
      "        145988, 145989, 145990, 145991,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0])\n",
      "a2b2[-2]:  tensor([145979, 145980, 145981, 145982, 145983, 145984, 145985, 145986, 145987,\n",
      "        145988, 145989, 145990, 145991,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0])\n"
     ]
    }
   ],
   "source": [
    "print(\"a2b[-1]: \", a2b[-1])\n",
    "print(\"\\na2b[-2]:  \", a2b[-2])\n",
    "print(\"a2b2[-1]:  \",a2b2[-1])\n",
    "\n",
    "print(\"\\na2b[-3]:  \", a2b[-3])\n",
    "print(\"a2b2[-2]: \", a2b2[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-effort",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Resources:\n",
    "\n",
    "- https://github.com/Open-Catalyst-Project/ocp/blob/6604e7130ea41fabff93c229af2486433093e3b4/ocpmodels/preprocessing/atoms_to_graphs.py\n",
    "- https://github.com/Open-Catalyst-Project/ocp/blob/master/scripts/preprocess_ef.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
