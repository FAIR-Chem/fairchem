
Scaling Dimenet++
-----------------

1. Async SGD / BMUF etc.
    - Small batch training seems to work well. First verify this.
    - Implement BMUF or a variant.

2. Split the nodes, edges and triplets once at the beginning
    - Use `multiprocessing.spawn()` to spawn threads yourself instead of relying on nn.DataParallel
        - Can this be done within DDP? If not, will need to create process groups.
    - Implement tensor communication (from master to/from other processes).
    - Can we implemet the scatters in parallel across all nodes?

2. Better load balancing since #nodes << #edges << #triplets
    - For very large models, run the triplets in a loop => can support much bigger batches.
        - May need to make use of checkpointing.


######### TODOs #########
# 1. Use DataParallel around scatter
# 2. Experiment with models with small #channels, but more layers
#    within a block (larger num_before_skip, num_after_skip, & num_output_layers)
# 3. Can we use multiple GPUs for graph creation?
