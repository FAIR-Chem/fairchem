"""
Copyright (c) Facebook, Inc. and its affiliates.

This source code is licensed under the MIT license found in the
LICENSE file in the root directory of this source tree.
"""

import logging
import os
import time
import traceback
import sys
import torch
from yaml import dump

from ocpmodels.common import dist_utils
from ocpmodels.common.flags import flags
from ocpmodels.common.registry import registry
from ocpmodels.common.utils import (
    JOB_ID,
    auto_note,
    build_config,
    merge_dicts,
    move_lmdb_data_to_slurm_tmpdir,
    resolve,
    setup_imports,
    setup_logging,
    update_from_sbatch_py_vars,
    set_min_hidden_channels,
)
from ocpmodels.common.orion_utils import (
    continue_orion_exp,
    load_orion_exp,
    sample_orion_hparams,
)
from ocpmodels.trainers import BaseTrainer

# os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
torch.multiprocessing.set_sharing_strategy("file_system")


def print_warnings():
    warnings = [
        "`max_num_neighbors` is set to 40. This should be tuned per model.",
        "`tag_specific_weights` is not handled for "
        + "`regress_forces: direct_with_gradient_target` in compute_loss()",
    ]
    print("\n" + "-" * 80 + "\n")
    print("üõë  OCP-DR-Lab Warnings (nota benes):")
    for warning in warnings:
        print(f"  ‚Ä¢ {warning}")
    print("Remove warnings when they are fixed in the code/configs.")
    print("\n" + "-" * 80 + "\n")


def wrap_up(args, start_time, error=None, signal=None, trainer=None):
    total_time = time.time() - start_time
    logging.info(f"Total time taken: {total_time}")
    if trainer and trainer.logger is not None:
        trainer.logger.log({"Total time": total_time})

    if args.distributed:
        print(
            "\nWaiting for all processes to finish with dist_utils.cleanup()...",
            end="",
        )
        dist_utils.cleanup()
        print("Done!")

    if "interactive" not in os.popen(f"squeue -hj {JOB_ID}").read():
        print("\nSelf-canceling SLURM job in 32s", JOB_ID)
        os.popen(f"sleep 32 && scancel {JOB_ID}")

    if trainer and trainer.logger:
        trainer.logger.finish(error or signal)


if __name__ == "__main__":
    error = signal = orion_exp = orion_trial = trainer = None
    orion_race_condition = False
    hparams = {}

    setup_logging()

    parser = flags.get_parser()
    args, override_args = parser.parse_known_args()
    args = update_from_sbatch_py_vars(args)
    if args.logdir:
        args.logdir = resolve(args.logdir)

    # -- Build config

    args.wandb_name = "alvaro-carbonero-math"
    args.wandb_project = "ocp-alvaro"
    args.test_ri = True
    args.mode = "train"
    args.graph_rewiring = "remove-tag-0"
    args.cp_data_to_tmpdir = True
    args.config = "indfaenet-is2re-10k"
    args.frame_averaging = "2D"
    args.fa_frames = "se3-random"

    trainer_config = build_config(args, override_args)

    if dist_utils.is_master():
        trainer_config = move_lmdb_data_to_slurm_tmpdir(trainer_config)
    dist_utils.synchronize()

    trainer_config["dataset"] = dist_utils.broadcast_from_master(
        trainer_config["dataset"]
    )

    trainer_config["model"]["edge_embed_type"] = "all_rij"
    trainer_config["model"]["mp_type"] = "updownscale"
    trainer_config["model"]["phys_embeds"] = True
    trainer_config["model"]["tag_hidden_channels"] = 32
    trainer_config["model"]["pg_hidden_channels"] = 64
    trainer_config["model"]["energy_head"] = "weighted-av-final-embeds"
    trainer_config["model"]["complex_mp"] = False
    trainer_config["model"]["graph_norm"] = True
    trainer_config["model"]["hidden_channels"] = 352
    trainer_config["model"]["num_filters"] = 448
    trainer_config["model"]["num_gaussians"] = 99
    trainer_config["model"]["num_interactions"] = 6
    trainer_config["model"]["second_layer_MLP"] = True
    trainer_config["model"]["skip_co"] = "concat"
    # trainer_config["model"]["transformer_out"] = False
    trainer_config["model"]["afaenet_gat_mode"] = "v1"
    # trainer_config["model"]["disconnected_mlp"] = True

    # trainer_config["optim"]["batch_sizes"] = 256
    # trainer_config["optim"]["eval_batch_sizes"] = 256
    trainer_config["optim"]["lr_initial"] = 0.0019
    trainer_config["optim"]["scheduler"] = "LinearWarmupCosineAnnealingLR"
    trainer_config["optim"]["max_epochs"] = 20
    trainer_config["optim"]["eval_every"] = 0.4

    # -- Initial setup

    setup_imports()
    print("\nüö© All things imported.\n")
    start_time = time.time()

    try:
        # -- Orion

        if args.orion_exp_config_path and dist_utils.is_master():
            orion_exp = load_orion_exp(args)
            hparams, orion_trial = sample_orion_hparams(orion_exp, trainer_config)

            if hparams.get("orion_race_condition"):
                logging.warning("\n\n ‚õîÔ∏è Orion race condition. Stopping here.\n\n")
                wrap_up(args, start_time, error, signal)
                sys.exit()

        hparams = dist_utils.broadcast_from_master(hparams)
        if hparams:
            print("\nüíé Received hyper-parameters from Orion:")
            print(dump(hparams), end="\n")
            trainer_config = merge_dicts(trainer_config, hparams)

        # -- Setup trainer
        trainer_config = continue_orion_exp(trainer_config)
        trainer_config = auto_note(trainer_config)
        trainer_config = set_min_hidden_channels(trainer_config)

        try:
            cls = registry.get_trainer_class(trainer_config["trainer"])
            trainer: BaseTrainer = cls(**trainer_config)
        except Exception as e:
            traceback.print_exc()
            logging.warning(f"\nüíÄ Error in trainer initialization: {e}\n")
            signal = "trainer_init_error"

        if signal is None:
            task = registry.get_task_class(trainer_config["mode"])(trainer_config)
            task.setup(trainer)
            print_warnings()

            # -- Start Training

            signal = task.run()

        # -- End of training

        # handle job preemption / time limit
        if signal == "SIGTERM":
            print("\nJob was preempted. Wrapping up...\n")
            if trainer:
                trainer.close_datasets()

        dist_utils.synchronize()

        objective = dist_utils.broadcast_from_master(
            trainer.objective if trainer else None
        )

        if orion_exp is not None:
            if objective is None:
                if signal == "loss_is_nan":
                    objective = 1e12
                    print("Received NaN objective from worker. Setting to 1e12.")
                if signal == "trainer_init_error":
                    objective = 1e12
                    print(
                        "Received trainer_init_error from worker.",
                        "Setting objective to 1e12.",
                    )
            if objective is not None:
                orion_exp.observe(
                    orion_trial,
                    [{"type": "objective", "name": "energy_mae", "value": objective}],
                )
            else:
                print("Received None objective from worker. Skipping observation.")

    except Exception:
        error = True
        print(traceback.format_exc())

    finally:
        wrap_up(args, start_time, error, signal, trainer=trainer)
