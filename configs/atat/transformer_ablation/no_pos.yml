includes:
  - configs/atat/transformer_ablation/base.yml

model:
  name: transformer
  num_elements: 100
  embed_dim: 128
  hidden_dim: 128
  dropout: 0.
  num_layers: 12
  num_heads: 8
  otf_graph: True
  rbf_radius: 15.
  num_gaussians: 64
  trainable_rbf: True
  num_pair_embed_layers: 2
  pair_embed_style: shared
  gate_pair_embed: True
  output_layers: 4
  gate_output: False
  avg_atoms: 60
  pos_emb_style: none

optim:
  batch_size: 64
  eval_batch_size: 64
  load_balancing: atoms
  num_workers: 16
  lr_initial: 0.0004

  optimizer: AdamW
  optimizer_params:
    weight_decay: 0.001
  scheduler: ReduceLROnPlateau
  mode: min
  factor: 0.9
  patience: 100
  max_epochs: 500
  force_coefficient: 10
  energy_coefficient: 1
  ema_decay: 0.999
  clip_grad_norm: 10
  loss_energy: mae
  loss_force: l2mae
