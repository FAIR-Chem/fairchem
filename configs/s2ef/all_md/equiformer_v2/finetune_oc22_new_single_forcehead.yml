dataset:
  train:
    src: /large_experiments/opencatalyst/data/oc22/2022_06_16/s2ef/train_ef/
    metadata_path: /checkpoint/bmwood/nima_checkpoints/oc22_metadata/oc22_metadata/train_metadata.npz
    key_mapping:
      y: energy
      force: forces
  val:
    src: /large_experiments/opencatalyst/data/oc22/2022_06_16/s2ef/val_id_30k/
    metadata_path: /large_experiments/opencatalyst/data/oc22/2022_06_16/s2ef/val_id_30k/metadata.npz
    sample_n: 10
  dataset_name: oc22
  format: mt_lmdb
  transforms:
    oc22_transform:
      dataset_name: oc22
    normalizer:
      energy:
        mean: 0.0
        stdev: 25.229595396538468
      forces:
        mean: 0.0
        stdev: 0.25678861141204834
    element_references:
      energy:
        file: /private/home/bmwood/ocp_results/fm_ocp/ocp/configs/oc22/linref/oc22_linfit_coeffs.npz

trainer: forces

model:
  name: finetune_hydra
  finetune_config:
    mode: RETAIN_BACKBONE_ONLY
    starting_checkpoint: "./checkpoints/2024-08-07-20-20-16-test/checkpoint.pt"
    heads:
      forces:
        module: equiformer_v2_force_head


logger:
  name: wandb

optim:
  batch_size: 5
  eval_batch_size: 2
  num_workers: 0
  lr_initial: 0.0025
  optimizer: AdamW
  optimizer_params: {"amsgrad": True,weight_decay: 0.0}
  eval_every: 190
  max_epochs: 50
  force_coefficient: 20
  scheduler: "Null"
  energy_coefficient: 1
  clip_grad_norm: 20
  loss_energy: mae
  loss_force: l2mae

outputs:
  forces:
    irrep_dim: 1
    level: atom
    train_on_free_atoms: True
    eval_on_free_atoms: True

loss_functions:
  - forces:
      fn: l2mae
      coefficient: 100

evaluation_metrics:
  metrics:
    forces:
      - mae
      - cosine_similarity
      - magnitude_error
  primary_metric: forces_mae