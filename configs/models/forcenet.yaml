default:
  model:
    name: forcenet
    decoder_type: mlp
    num_freqs: 50
    training: true
    predict_forces: false
    num_interactions: 5
    cutoff: 6.0
    basis: "sphallmul"
    ablation: "none"
    depth_mlp_edge: 2
    depth_mlp_node: 1
    activation_str: "swish"
    decoder_activation_str: "swish"
    feat: "full"
    hidden_channels: 512
    decoder_hidden_channels: 512
    max_n: 3
    use_pbc: True
    # drlab attributes:
    tag_hidden_channels: 0 # 64
    pg_hidden_channels: 0 # 32 -> period & group embedding hidden channels
    phys_embeds: False # True
    phys_hidden_channels: 0
    energy_head: False # can be {False, weighted-av-initial-embeds, weighted-av-final-embeds, pooling, graclus, random}
  optim:
    batch_size: 8
    eval_batch_size: 8
    num_workers: 8
    energy_coefficient: 0
    warmup_factor: 0.2
    warmup_steps: 9375
    lr_gamma: 0.1
    lr_initial: 0.0005
    lr_milestones: # steps at which lr_initial <- lr_initial * lr_gamma
      - 15625
      - 25000
      - 31250

# -------------------
# -----  IS2RE  -----
# -------------------

is2re:
  10k:
    optim:
      # *** Important note ***
      #   The total number of gpus used for this run was 8.
      #   If the global batch size (num_gpus * batch_size) is modified
      #   the lr_milestones and warmup_steps need to be adjusted accordingly.
      max_epochs: 20
  100k:
    optim:
      max_epochs: 20
  all:
    model:
      num_interactions: 4
    optim:
      max_epochs: 8 # 20


# ------------------
# -----  S2EF  -----
# ------------------

s2ef:
  200k:

  2M:

  20M:

  all:
