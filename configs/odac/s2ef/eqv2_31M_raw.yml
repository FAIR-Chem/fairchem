dataset:
  train:
    src: data_odac/s2ef/train
    normalize_labels: True
    target_mean: 0.5514033085536203
    target_std: 15.554063738073438
    grad_target_mean: 0.0
    grad_target_std: 0.04262716323137283
    # lin_ref: configs/odac/s2ef_lin_ref.npz
    target: raw_y

  val:
    src: data_odac/s2ef/val_0.1
    # lin_ref: configs/odac/s2ef_lin_ref.npz
    target: raw_y

logger:
  name: wandb

task:
  dataset: lmdb
  train_on_free_atoms: True
  eval_on_free_atoms: True
  primary_metric: forces_mae

  relaxation_steps: 125
  relaxation_fmax: 0.05
  write_pos: True
  relax_dataset:
    src: data_odac/is2r/val
  relax_opt:
    name: lbfgs
    maxstep: 0.04
    memory: 50
    damping: 1.0
    alpha: 70.0

trainer: equiformerv2_forces

model:
  name: equiformer_v2

  use_pbc:                  True
  regress_forces:           True
  otf_graph:                True
  max_neighbors:            20
  max_radius:               8.0
  max_num_elements:         100

  num_layers:               8
  sphere_channels:          128
  attn_hidden_channels:     64              # [64, 96] This determines the hidden size of message passing. Do not necessarily use 96.
  num_heads:                8
  attn_alpha_channels:      64              # Not used when `use_s2_act_attn` is True.
  attn_value_channels:      16
  ffn_hidden_channels:      128
  norm_type:                'layer_norm_sh' # ['rms_norm_sh', 'layer_norm', 'layer_norm_sh']

  lmax_list:                [4]
  mmax_list:                [2]
  grid_resolution:          18              # [18, 16, 14, None] For `None`, simply comment this line.

  num_sphere_samples:       128

  edge_channels:            128
  use_atom_edge_embedding:  True
  distance_function:        'gaussian'
  num_distance_basis:       512         # not used

  attn_activation:          'silu'
  use_s2_act_attn:          False       # [False, True] Switch between attention after S2 activation or the original EquiformerV1 attention.
  ffn_activation:           'silu'      # ['silu', 'swiglu']
  use_gate_act:             False       # [False, True] Switch between gate activation and S2 activation
  use_grid_mlp:             True        # [False, True] If `True`, use projecting to grids and performing MLPs for FFNs.

  alpha_drop:               0.1         # [0.0, 0.1]
  drop_path_rate:           0.1         # [0.0, 0.05]
  proj_drop:                0.0

  weight_init:              'uniform'    # ['uniform', 'normal']

  avg_num_nodes:            192.561
  avg_degree:               21.024127419363214

  use_energy_lin_ref: True
  load_energy_lin_ref: True

  # energy_lin_ref_path: configs/odac/s2ef_lin_ref_offset.npz

optim:
  batch_size:                   3
  eval_batch_size:              3
  grad_accumulation_steps:      1         # gradient accumulation: effective batch size = `grad_accumulation_steps` * `batch_size` * (num of GPUs)
  load_balancing: atoms
  num_workers: 8
  lr_initial:                   0.0004    # [0.0002, 0.0004], eSCN uses 0.0008 for batch size 96

  optimizer: AdamW
  optimizer_params:
    weight_decay: 0.3
  scheduler: LambdaLR
  scheduler_params:
    lambda_type: cosine
    warmup_factor: 0.2
    warmup_epochs: 0.01
    lr_min_factor: 0.01

  max_epochs: 3
  force_coefficient: 200
  energy_coefficient: 1
  clip_grad_norm: 100
  ema_decay: 0.999
  loss_energy: mae
  loss_force: l2mae

  eval_every: 5000
