includes:
  - configs/oc22/s2ef/base.yml

model:
  name: gemnet_oc
  num_spherical: 7
  num_radial: 128
  num_blocks: 4
  emb_size_atom: 256
  emb_size_edge: 512
  emb_size_trip_in: 64
  emb_size_trip_out: 64
  emb_size_quad_in: 32
  emb_size_quad_out: 32
  emb_size_aint_in: 64
  emb_size_aint_out: 64
  emb_size_rbf: 16
  emb_size_cbf: 16
  emb_size_sbf: 32
  num_before_skip: 2
  num_after_skip: 2
  num_concat: 1
  num_atom: 3
  num_output_afteratom: 3
  cutoff: 12.0
  cutoff_qint: 12.0
  cutoff_aeaint: 12.0
  cutoff_aint: 12.0
  max_neighbors: 30
  max_neighbors_qint: 8
  max_neighbors_aeaint: 20
  max_neighbors_aint: 1000
  rbf:
    name: gaussian
  envelope:
    name: polynomial
    exponent: 5
  cbf:
    name: spherical_harmonics
  sbf:
    name: legendre_outer
  extensive: True
  output_init: HeOrthogonal
  activation: silu

  regress_forces: True
  direct_forces: True
  forces_coupled: False
  otf_graph: True

  quad_interaction: True
  atom_edge_interaction: True
  edge_atom_interaction: True
  atom_interaction: True

  num_atom_emb_layers: 2
  num_global_out_layers: 2
  qint_tags: [1, 2]

optim:
  batch_size: 16
  eval_batch_size: 16
  load_balancing: atoms
  eval_every: 5000
  num_workers: 2
  lr_initial: 1.e-4
  optimizer: AdamW
  optimizer_params: {"amsgrad": True}
  warmup_steps: -1 # don't warm-up the learning rate
  # warmup_factor: 0.2
  lr_gamma: 0.8
  # Following calculation is for an effective batch size of 16 x 16 GPUs = 256
  # and a dataset size of 8225293 (1 epoch = 32130 steps).
  # The older dataset had 6140155 points (1 epoch = 23984 steps).
  lr_milestones: # steps at which lr_initial <- lr_initial * lr_gamma
    - 64000 # ~2 epochs
    - 96000 # ~3 epochs
    - 128000 # ~4 epochs
    - 160000 # ~5 epochs
    - 192000 # ~6 epochs
    - 224000 # ~7 epochs
    - 256000 # ~8 epochs
    - 288000 # ~9 epochs
    - 320000 # ~10 epochs
    - 336000 # ~10.5 epochs
    - 352000 # ~11 epochs
    - 368000 # ~11.5 epochs
    - 384000 # ~12 epochs
    - 400000 # ~12.5 epochs
    - 416000 # ~13 epochs
    - 432000 # ~13.5 epochs
    - 448000 # ~14 epochs
    - 464000 # ~14.5 epochs
  max_epochs: 15
  ema_decay: 0.999
  clip_grad_norm: 10
  weight_decay: 0  # 2e-6 (TF weight decay) / 1e-4 (lr) = 2e-2
  loss_energy: mae
  loss_force: l2mae
  force_coefficient: 100
  energy_coefficient: 1
