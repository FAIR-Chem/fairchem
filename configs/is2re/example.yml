# Example config for training models for IS2RE.

trainer: energy                                                                 # 'energy' or 'forces'

task:
  # The code currently supports 'lmdb' and 'oc22_lmdb' for both IS2RE and S2EF.
  #
  # To train models on adsorption energy (as in OC20), use `lmdb`.
  # To train models on total DFT energy, use `oc22_lmdb`.
  #
  # Can use 'single_point_lmdb' or 'trajectory_lmdb' for backward compatibility.
  # 'single_point_lmdb' was for training IS2RE models, and 'trajectory_lmdb' was
  # for training S2EF models.
  dataset: lmdb                                                                 # 'lmdb' or 'oc22_lmdb'
  # This is an optional parameter specifying the val metric to watch for
  # improvement to decide when to save checkpoints.
  # By default, this is:
  #   'energy_force_within_threshold' for S2EF,
  #   'energy_mae' for IS2RE,
  #   'average_distance_within_threshold' for IS2RS.
  primary_metric: energy_mae

dataset:
  train:
    # Path to training set LMDB
    src: data/is2re/all/train/data.lmdb
    # If we want to normalize each target value, i.e. subtract the mean and
    # divide by standard deviation, then those 'target_mean' and 'target_std'
    # statistics need to be specified here for the train split.
    normalize_labels: True                                                      # True or False
    # These stats are for OC20 IS2RE.
    target_mean: -1.525913953781128
    target_std: 2.279365062713623
    # If we want to train OC20 on total energy, a path to OC20 reference
    # energies `oc20_ref` must be specificed to unreference existing OC20 data.
    # OC22 by default trains on total energy, so this flag is not necessary.
    total_energy: False                                                         # True or False
    oc20_ref: False                                                             # True or False
    # If we want to train on total energies and use a linear reference
    # normalization scheme, we must specify the path to the per-element
    # coefficients in a `.npz` format.
    lin_ref: False
  val:
    # Path to val set LMDB
    src: data/is2re/all/val_id/data.lmdb
  test:
    # Path to test set LMDB
    src: data/is2re/all/test_id/data.lmdb

logger: tensorboard                                                             # 'wandb' or 'tensorboard'

model:
  name: gemnet_t
  # Model attributes go here, e.g. no. of layers, no. of hidden channels,
  # embedding functions, cutoff radius, no. of neighbors, etc.
  # This list of params will look different depending on the model.
  #
  # 'otf_graph' specifies whether graph edges should be computed on the fly
  # or they already exist in the preprocessed LMDBs. If unsure, set it to True.
  otf_graph: True                                                               # True or False
  # All models in OCP can be used to predict just energies, or both energies and
  # forces. For IS2RE, we don't need forces, so 'regress_forces' is False.
  regress_forces: False                                                         # True or False

optim:
  # Batch size per GPU for training.
  # Note that effective batch size will be 'batch_size' x no. of GPUs.
  batch_size: 8
  # Batch size per GPU for evaluation.
  # Note that effective batch size will be 'eval_batch_size' x no. of GPUs.
  eval_batch_size: 8
  # No. of subprocesses to use for dataloading, pass as an arg to
  # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader.
  num_workers: 2
  # After how many updates to run evaluation on val during training.
  # If unspecified, defaults to 1 epoch.
  eval_every: 5000
  # Loss function to use for energies. Defaults to 'mae'.
  loss_energy: mae                                                              # 'mae' or 'mse'
  # Optimizer to use from torch.optim.
  # Default is https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html.
  optimizer: AdamW
  # Learning rate. Passed as an `lr` argument when initializing the optimizer.
  lr_initial: 1.e-4
  # Additional args needed to initialize the optimizer.
  optimizer_params: {"amsgrad": True}
  # Weight decay to use. Passed as an argument when initializing the optimizer.
  weight_decay: 0
  # Learning rate scheduler. Should work for any scheduler specified in
  # in torch.optim.lr_scheduler: https://pytorch.org/docs/stable/optim.html
  # as long as the relevant args are specified here.
  #
  # For example, for ReduceLROnPlateau, we specify `mode`, `factor`, `patience`.
  # https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html
  #
  # Note that if task.primary_metric specified earlier in the config is a metric
  # where higher is better (e.g. 'energy_force_within_threshold' or
  # 'average_distance_within_threshold'), `mode` should be 'max' since we'd want
  # to step LR when the metric has stopped increasing. Vice versa for energy_mae
  # or forces_mae or loss.
  #
  # If you don't want to use a scheduler, set it to 'Null' (yes type that out).
  # This is for legacy reasons. If scheduler is unspecified, it defaults to
  # 'LambdaLR': warming up the learning rate to 'lr_initial' and then stepping
  # it at pre-defined set of steps. See the DimeNet++ config for how to do this.
  scheduler: ReduceLROnPlateau
  mode: min
  factor: 0.8
  patience: 3
  # No. of epochs to train for.
  max_epochs: 100
  # Exponential moving average of parameters. 'ema_decay' is the decay factor.
  ema_decay: 0.999
  # Max norm of gradients for clipping. Uses torch.nn.utils.clip_grad_norm_.
  clip_grad_norm: 10

slurm:
  constraint: "rtx_6000"
