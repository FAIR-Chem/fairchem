includes:
  - configs/is2re/all/base.yml

model:
  name: new_forcenet
  decoder_type: mlp
  num_freqs: 50
  predict_forces: false
  training: true
  num_interactions: 5
  cutoff: 6
  basis: "sphallmul"
  ablation: "none"
  depth_mlp_edge: 2
  depth_mlp_node: 1
  activation_str: "swish"
  decoder_activation_str: "swish"
  feat: "full"
  hidden_channels: 512
  decoder_hidden_channels: 512
  max_n: 3
  use_pbc: True
  # drlab attributes:
  tag_hidden_channels: 0 # 64
  pg_hidden_channels: 0 # 32 -> period & group embedding hidden channels
  phys_embeds: False # True
  phys_hidden_channels: 0
  graph_rewiring:
    False # {false, remove-tag-0, one-supernode-per-graph,
    # one-supernode-per-atom-type, one-supernode-per-atom-type-dist}
  energy_head: False # can be {False, weighted-av-initial-embeds,
                                              # weighted-av-final-embeds, pooling, graclus, random}

# *** Important note ***
#   The total number of gpus used for this run was 8.
#   If the global batch size (num_gpus * batch_size) is modified
#   the lr_milestones and warmup_steps need to be adjusted accordingly.

optim:
  batch_size: 8
  eval_batch_size: 8
  num_workers: 8
  lr_initial: 0.0005
  max_epochs: 8 # 20
  energy_coefficient: 0
  lr_gamma: 0.1
  lr_milestones: # steps at which lr_initial <- lr_initial * lr_gamma
    - 15625
    - 25000
    - 31250
  warmup_steps: 9375
  warmup_factor: 0.2
