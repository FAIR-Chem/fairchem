# more epochs, larger batch size, explore fanet: larger model & skip-co & mlp_rij
job:
  mem: 32GB
  cpus: 4
  gres: gpu:1
  time: 1:00:00
  partition: main
  code_loc: /home/mila/s/schmidtv/ocp-project/run-repos/ocp-2
  env: ocp-a100

default:
  wandb_project: ocp-qm
  config: schnet-qm9-all
  mode: train
  test_ri: true
  wandb_tags: qm9, orion-debug
  phys_hidden_channels: 0
  phys_embeds: False
  energy_head: False
  pg_hidden_channels: 0
  tag_hidden_channels: 0
  frame_averaging: ""
  cp_data_to_tmpdir: true
  optim:
    batch_size: 64
    warmup_steps: 3000
    lr_initial: 0.0002
    # parameters EMA
    ema_decay: 0.999
    # exp. decay to 0.01 * lr_initial in 1000000 steps
    decay_steps: max_steps
    decay_rate: 0.05 # at the end of training, lr is decay_rate*lr_initial
    # max_epochs = ref_steps[3e6] / (n_train[110 000] / ref_batch_size[32])
    max_epochs: -1
    max_steps: 3000000
  note:
    model: name, num_gaussians, hidden_channels, num_filters, num_interactions, phys_embeds, pg_hidden_channels, phys_hidden_channels
    optim: batch_size, lr_initial
    _root_: frame_averaging, fa_frames

orion:
  # Remember to change the experiment name if you change anything in the search space
  _meta_:
    n_runs: 2
    unique_exp_name: ocp-qm9-orion-debug
  optim:
    batch_size: uniform(32, 1024, discrete=True)
    lr_initial: loguniform(1e-5, 5e-3, precision=2)
    max_steps: fidelity(1e4, 1e6, base=5e5)
  model:
    num_gaussians: uniform(16, 200, base=20, discrete=True)
    hidden_channels: uniform(32, 512, discrete=True)
    num_filters: uniform(32, 512, discrete=True)
    num_interactions: uniform(1, 7, discrete=True)
    phys_embeds: choices([True, False])