# more epochs, larger batch size, explore fanet: larger model & skip-co & mlp_rij
job:
  mem: 12GB
  cpus: 4
  gres: gpu:16gb:1
  partition: main

default:
  wandb_project: ocp-qm
  config: fanet-qm9-all
  mode: train
  test_ri: true
  wandb_tags: qm9, fanet-qm9-v1.0.2-continued
  log_train_every: 100
  optim:
    warmup_steps: 2000
    # parameters EMA
    ema_decay: 0.999
    decay_steps: max_steps
    scheduler: LinearWarmupCosineAnnealingLR
    batch_size: 64
    initial_lr: 0.001
    max_epochs: 1500
  note:
    model: name, num_gaussians, hidden_channels, num_filters, num_interactions, phys_embeds, pg_hidden_channels, phys_hidden_channels, energy_head, edge_embed_type, mp_type, graph_norm
    optim: batch_size, lr_initial
    _root_: frame_averaging, fa_frames
  orion_mult_factor:
    value: 32
    targets: hidden_channels, num_filters, pg_hidden_channels, phys_hidden_channels
  frame_averaging: 3D
  fa_frames: random
  model:
    edge_embed_type: all_rij
    energy_head: weighted-av-initial-embeds
    graph_norm: True
    hidden_channels: 416
    max_num_neighbors: 40
    mp_type: updownscale
    num_filters: 512
    num_gaussians: 100
    num_interactions: 3
    otf_graph: false
    pg_hidden_channels: 0
    phys_embeds: false
    phys_hidden_channels: 0
    second_layer_MLP: false
    skip_co: false
    tag_hidden_channels: 0
    use_pbc: false
    regress_forces: ""


runs:
  - {}
  - model:
      mp_type: base_with_att