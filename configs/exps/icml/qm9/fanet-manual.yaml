# scheduler reduce lr on plateau
job:
  mem: 12GB
  cpus: 4
  gres: gpu:16gb:1
  partition: long

default:
  wandb_project: ocp-qm
  config: fanet-qm9-all
  mode: train
  test_ri: true
  wandb_tags: qm9, fanet-qm9-v1.0.2-continued
  log_train_every: 100
  note:
    model: name, num_gaussians, hidden_channels, num_filters, num_interactions, phys_embeds, pg_hidden_channels, energy_head, mp_type
    optim: batch_size
    _root_: frame_averaging, fa_frames
  frame_averaging: 3D
  fa_frames: random
  optim:
    warmup_steps: 3000
    # parameters EMA
    ema_decay: 0.999
    batch_size: 64
    initial_lr: 0.0005
    max_epochs: 1500
    loss_energy: mse
    loss_force: mse
    # early stopping
    es_patience: 50
    es_min_abs_change: 0.000001
    es_warmup_epochs: 500
    # all below is for the ReduceLROnPlateau scheduler
    scheduler: ReduceLROnPlateau
    mode: min
    factor: 0.75
    threshold: 0.0001
    threshold_mode: abs
    min_lr: 0.000001
    verbose: true
    patience: 10
  model:
    cutoff: 5.0
    edge_embed_type: all_rij
    energy_head: weighted-av-final-embeds
    graph_norm: True
    hidden_channels: 350
    max_num_neighbors: 40
    mp_type: updownscale
    num_filters: 256
    num_gaussians: 50
    num_interactions: 5
    otf_graph: false
    pg_hidden_channels: 16
    phys_embeds: true
    phys_hidden_channels: 0
    second_layer_MLP: false
    skip_co: false
    tag_hidden_channels: 0
    use_pbc: false
    regress_forces: ""


runs:
  - {}
  - model:
      mp_type: base_with_att
  - model:
      cutoff: 6.0
  - optim:
      batch_size: 128
  - model:
      energy_head: ""
  - model:
      energy_head: "weighted-av-initial-embeds"
  - model:
      num_interactions: 4
      num_gaussians: 20
      hidden_channels: 512
      num_filters: 512
  - model:
      mp_type: updownscale_base
      num_interactions: 4
      num_gaussians: 20
      hidden_channels: 512
      num_filters: 512
