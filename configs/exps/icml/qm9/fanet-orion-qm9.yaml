# more epochs, larger batch size, explore fanet: larger model & skip-co & mlp_rij
job:
  mem: 8GB
  cpus: 4
  gres: gpu:1
  time: 02:50:00
  partition: long

default:
  wandb_project: ocp-4
  config: fanet-qm9-all
  mode: train
  test_ri: true
  wandb_tags: qm9, orion
  log_train_every: 100
  optim:
    warmup_steps: 2000
    # parameters EMA
    ema_decay: 0.999
    decay_steps: max_steps
    scheduler: LinearWarmupCosineAnnealingLR
    batch_size: 64
  note:
    model: name, num_gaussians, hidden_channels, num_filters, num_interactions, phys_embeds, pg_hidden_channels, phys_hidden_channels, energy_head, edge_embed_type, mp_type, graph_norm
    optim: batch_size, lr_initial
    _root_: frame_averaging, fa_frames
  orion_mult_factor:
    value: 32
    targets: hidden_channels, num_filters, pg_hidden_channels, phys_hidden_channels
  frame_averaging: 3D
  fa_frames: random
  model:
    edge_embed_type: all_rij

orion:
  # Remember to change the experiment name if you change anything in the search space
  n_jobs: 20

  unique_exp_name: fanet-qm9-v1.0.1

  space:
    optim/max_epochs: fidelity(30, 300, base=6)
    optim/lr_initial: loguniform(1e-4, 5e-3, precision=2)
    model/graph_norm: choices([True, False])
    model/energy_head: choices(["", "weighted-av-final-embeds", "weighted-av-initial-embeds"])
    model/hidden_channels: uniform(5, 16, discrete=True)
    model/mp_type: choices(["simple", "base", "sfarinet", "updownscale", "updownscale_base", "base_with_att", "att", "local_env", "updown_local_env"])
    model/num_filters: uniform(3, 16, discrete=True)
    model/num_gaussians: uniform(20, 150, discrete=True)
    model/num_interactions: uniform(1, 7, discrete=True)
    model/pg_hidden_channels: uniform(0, 2, discrete=True)
    model/phys_embeds: choices([True, False])
  algorithms:
    asha:
      seed: 123
      num_rungs: 4
      num_brackets: 2
