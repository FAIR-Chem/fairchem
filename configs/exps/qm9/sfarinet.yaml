# more epochs, larger batch size, explore fanet: larger model & skip-co & mlp_rij
job:
  mem: 48GB
  cpus: 8
  gres: gpu:1
  time: 24:00:00
  partition: long
  code_loc: /home/mila/s/schmidtv/ocp-project/run-repos/ocp-1
  env: ocp-a100

default:
  wandb_project: ocp-qm
  mode: train
  test_ri: true
  wandb_tags: qm9
  phys_hidden_channels: 0
  phys_embeds: False
  energy_head: False
  pg_hidden_channels: 0
  tag_hidden_channels: 0
  frame_averaging: ""
  default:
  model:
    name: sfarinet
    act: swish
    num_filters: 128
    num_gaussians: 100
    cutoff: 6.0
    use_pbc: False
    regress_forces: False # can be in {"from_energy", "direct", "direct_with_gradient_target"}
    # drlab attributes:
    hidden_channels: 256
    num_interactions: 3
    tag_hidden_channels: 0 # 32
    pg_hidden_channels: 0 # 32 -> period & group embedding hidden channels
    phys_embeds: False # True
    phys_hidden_channels: 0
    energy_head: False # can be {False, weighted-av-initial-embeds, weighted-av-final-embeds, pooling, graclus, random}
  optim:
    batch_size: 512
    warmup_steps: 1000
    lr_initial: 0.002
    # parameters EMA
    ema_decay: 0.999
    # exp. decay to 0.01 * lr_initial in 1000000 steps
    decay_steps: 250000
    decay_rate: 0.01
    # max_epochs = ref_steps[3e6] / (n_train[110 000] / ref_batch_size[32])
    max_epochs: 872


runs:
  - config: sfarinet-qm9-all
    note: Sfarinet qm9 baseline
  - config: sfarinet-qm9-all
    note: Sfarinet qm9 PhAST
    model:
      phys_embeds: True
      pg_hidden_channels: 64
  - config: sfarinet-qm9-all
    note: Sfarinet qm9 PhAST-Large
    model:
      phys_embeds: True
      pg_hidden_channels: 64
      hidden_channels: 512
      num_interactions: 5
  - config: sfarinet-qm9-all
    note: Sfarinet qm9 PhAST-XLarge
    model:
      phys_embeds: True
      phys_hidden_channels: 128
      pg_hidden_channels: 128
      hidden_channels: 512
      num_interactions: 6
      num_filters: 256
      num_gaussians: 150