# trainset has 4068193 samples
job:
  mem: 12GB
  cpus: 4
  gres: gpu:1
  partition: long
  code_loc: /home/mila/s/schmidtv/ocp-project/run-repos/ocp-1
  env: ocp-a100

default:
  config: schnet-qm7x-all
  wandb_project: ocp-qm
  mode: train
  test_ri: true
  wandb_tags: qm7x, no_metrics_denorm
  phys_hidden_channels: 0
  phys_embeds: False
  energy_head: False
  pg_hidden_channels: 0
  tag_hidden_channels: 0
  frame_averaging: ""
  cp_data_to_tmpdir: true
  no_metrics_denorm: true
  note:
    task: name
    model: name, num_gaussians, hidden_channels, num_filters, num_interactions
    optim: batch_size, lr_initial
  optim:
    batch_size: 10
    warmup_steps: 3000
    lr_initial: 0.0001
    # parameters EMA
    # ema_decay: 0.999
    decay_steps: max_steps
    scheduler:
    decay_rate: 0.01
    max_steps: 2000000
    eval_every: 50000
  model:
    hidden_channels: 128
    num_filters: 128
    num_gaussians: 20
    num_interactions: 6
    cutoff: 5.0

# runs:
#   - optim:
#       ema_decay: 0.999
#   - optim:
#       scheduler: LinearWarmupCosineAnnealingLR
#     model:
#       cutoff: 6.0
#   - optim:
#       scheduler: LinearWarmupCosineAnnealingLR
#     model:
#       num_gaussians: 100
#   - optim:
#       scheduler: LinearWarmupCosineAnnealingLR
#     model:
#       num_filters: 256
#   - optim:
#       ema_decay: 0.999
#       scheduler: LinearWarmupCosineAnnealingLR

runs: # all above contributed positively to improve eval/val_ood/energy_mae.
      # so we're combining them here. + test with slightly larger batch size.
      # And with no_metrics_denorm.
  - optim:
      ema_decay: 0.999
      scheduler: LinearWarmupCosineAnnealingLR
    model:
      cutoff: 6.0
      num_filters: 256
      num_gaussians: 100
  - optim:
      batch_size: 32
      ema_decay: 0.999
      scheduler: LinearWarmupCosineAnnealingLR
    model:
      cutoff: 6.0
      num_filters: 256
      num_gaussians: 100
  - optim:
      batch_size: 512
      lr_initial: 0.0005
      ema_decay: 0.999
      scheduler: LinearWarmupCosineAnnealingLR
    model:
      cutoff: 6.0
      num_filters: 256
      num_gaussians: 100